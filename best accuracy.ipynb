{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2618264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Ensure the correct environment variable is set to avoid oneDNN warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_data 3 genres.csv')\n",
    "\n",
    "\n",
    "# Encode genres to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['Genre_Encoded'] = label_encoder.fit_transform(data['Genre'])\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['Lyrics'].astype(str), data['Genre_Encoded'], test_size=0.05, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(len(train_texts)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model.trainable = True  # Fine-tune BERT\n",
    "\n",
    "# Define additional layers\n",
    "inputs = {key: tf.keras.Input(shape=(128,), dtype=tf.int32, name=key) for key in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
    "bert_outputs = model(inputs).logits\n",
    "dropout_output = tf.keras.layers.Dropout(0.5)(bert_outputs)\n",
    "dense_output = tf.keras.layers.Dense(128, activation='relu')(dropout_output)\n",
    "final_output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(dense_output)\n",
    "dropout_model = tf.keras.Model(inputs=inputs, outputs=final_output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "metrics = ['accuracy']\n",
    "dropout_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = dropout_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = dropout_model.evaluate(val_dataset)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Predict on validation set\n",
    "val_predictions = dropout_model.predict(val_dataset)\n",
    "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(val_labels, val_pred_labels, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Assume val_labels and val_pred_labels are defined from previous steps\n",
    "conf_matrix = confusion_matrix(val_labels, val_pred_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = r'C:\\python\\pythonPtixiaki\\best model'\n",
    "dropout_model.save(model_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
